{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification using tf.keras_jay_JUN2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay-pm/Deep-Learning/blob/master/Image_Classification_using_tf_keras_jay_JUN2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ivXyUlo02NO",
        "colab_type": "text"
      },
      "source": [
        "**Image Classification using tf.keras**\n",
        "\n",
        "We will classify images of flowers by building an image classifier using tf.keras.Sequential model and load data using tf.keras.preprocessing.image.ImageDataGenerator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRh0dIzZzvqR",
        "colab_type": "text"
      },
      "source": [
        "*Import packages*\n",
        "\n",
        "- numpy: for matrix operations\n",
        "- os: for reading files and directory strcture\n",
        "- matplotlib.pyplot : for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF6K4Q_Wzzom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojHkdxLO1B45",
        "colab_type": "text"
      },
      "source": [
        "*Import TensorFlow and Keras Layers*\n",
        "\n",
        "- tensorflow: numerical computation framework\n",
        "- keras.layers : for building CNN and DNN layers\n",
        "- ImageDataGenerator: for image augmentation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovOz0oxv0EZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gO0GZTR01lx",
        "colab_type": "text"
      },
      "source": [
        "*Load the data*\n",
        "\n",
        "Let's download the flowers image data from URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deM7SwRd1Z8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "71667df8-ea22-43cd-ea78-59a4a65130dc"
      },
      "source": [
        "URL = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "\n",
        "zip_file = tf.keras.utils.get_file(origin=URL, fname=\"flower_photos.tgz\", extract=True)\n",
        "\n",
        "base_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "228818944/228813984 [==============================] - 5s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bJRGLPL166l",
        "colab_type": "text"
      },
      "source": [
        "The dataset we downloaded contains images of 5 types of flowers:\n",
        "\n",
        "    Rose\n",
        "    Daisy\n",
        "    Dandelion\n",
        "    Sunflowers\n",
        "    Tulips\n",
        "\n",
        "So, let's create the labels for these 5 classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1i-XhuB1zx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = ['roses', 'daisy', 'dandelion', 'sunflowers', 'tulips']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBLYY9Iy2ccg",
        "colab_type": "text"
      },
      "source": [
        "The dataset we have downloaded has following directory structure.\n",
        "\n",
        "  - flower_photos\n",
        "        diasy\n",
        "        dandelion\n",
        "        roses\n",
        "        sunflowers\n",
        "        tulips\n",
        "        \n",
        " \n",
        "As ythere are no folders containing training and validation data we will have to create our own training and validation set.\n",
        "\n",
        "The code below creates a train and a val folder each containing 5 folders (one for each type of flower). It then moves the images from the original folders to these new folders such that 80% of the images go to the training set and 20% of the images go into the validation set. In the end our directory will have the following structure:\n",
        "\n",
        "- flower_photos\n",
        "          diasy\n",
        "          dandelion\n",
        "          roses\n",
        "          sunflowers\n",
        "          tulips\n",
        "          train\n",
        "              daisy: [1.jpg, 2.jpg, 3.jpg ....]\n",
        "              dandelion: [1.jpg, 2.jpg, 3.jpg ....]\n",
        "              roses: [1.jpg, 2.jpg, 3.jpg ....]\n",
        "              sunflowers: [1.jpg, 2.jpg, 3.jpg ....]\n",
        "              tulips: [1.jpg, 2.jpg, 3.jpg ....]\n",
        "           val\n",
        "              daisy: [507.jpg, 508.jpg, 509.jpg ....]\n",
        "              dandelion: [719.jpg, 720.jpg, 721.jpg ....]\n",
        "              roses: [514.jpg, 515.jpg, 516.jpg ....]\n",
        "              sunflowers: [560.jpg, 561.jpg, 562.jpg .....]\n",
        "              tulips: [640.jpg, 641.jpg, 642.jpg ....]   \n",
        "\n",
        "Since we don't delete the original folders, they will still be in our flower_photos directory, but they will be empty. The code below also prints the total number of flower images we have for each type of flower."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLIucnK42bM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "9387e4ab-3a96-42a5-c70a-ad9da1184577"
      },
      "source": [
        "for cl in classes:\n",
        "  img_path = os.path.join(base_dir, cl)\n",
        "  images = glob.glob(img_path + '/*.jpg')\n",
        "  print(\"{}: {} Images\".format(cl, len(images)))\n",
        "  num_train = int(round(len(images)*0.8))\n",
        "  train, val = images[:num_train], images[num_train:]\n",
        "\n",
        "  for t in train:\n",
        "    if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n",
        "      os.makedirs(os.path.join(base_dir, 'train', cl))\n",
        "    shutil.move(t, os.path.join(base_dir, 'train', cl))\n",
        "\n",
        "  for v in val:\n",
        "    if not os.path.exists(os.path.join(base_dir, 'val', cl)):\n",
        "      os.makedirs(os.path.join(base_dir, 'val', cl))\n",
        "    shutil.move(v, os.path.join(base_dir, 'val', cl))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "roses: 641 Images\n",
            "daisy: 633 Images\n",
            "dandelion: 898 Images\n",
            "sunflowers: 699 Images\n",
            "tulips: 799 Images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZndWceZ4UMf",
        "colab_type": "text"
      },
      "source": [
        " let us set up the path for the training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bfVskIS4KAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNw8r_lg43Ye",
        "colab_type": "text"
      },
      "source": [
        "Overfitting generally occurs when we have small number of training examples. One way to fix this problem is to augment our dataset so that it has sufficient number of training examples. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\n",
        "\n",
        "In **tf.keras** we can implement this using the same **ImageDataGenerator** class we used before. We can simply pass  different transformations we would want to our dataset as a form of arguments and it will take care of applying it to the dataset during our training process. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVtoHa_n5CVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  Set Batch and Image Size\n",
        "batch_size = 100\n",
        "IMG_SHAPE = 150 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv1SkUCz6DFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply data augmentation on training set\n",
        "\n",
        "image_gen_train = ImageDataGenerator(\n",
        "                    rescale=1./255, \n",
        "                    rotation_range=45, \n",
        "                    width_shift_range=.15, \n",
        "                    height_shift_range=.15, \n",
        "                    horizontal_flip=True, \n",
        "                    zoom_range=0.5\n",
        "                    )\n",
        "\n",
        "\n",
        "train_data_gen = image_gen_train.flow_from_directory(\n",
        "                                                batch_size=batch_size, \n",
        "                                                directory=train_dir, \n",
        "                                                shuffle=True, \n",
        "                                                target_size=(IMG_SHAPE,IMG_SHAPE),\n",
        "                                                class_mode='sparse'\n",
        "                                                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZGn7Qkt7i-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# view few images\n",
        "plotImages([train_data_gen[0][0][0] for i in range(5)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOHFZdDz73oq",
        "colab_type": "text"
      },
      "source": [
        "**Create a Data Generator for the Validation Set**\n",
        "Data augmentation is only applied to training data. So we will use ImageDataGenerator to create a transformation that only rescales the images by 255. Then use the .flow_from_directory method to apply the above transformation to the images in our validation set. We need to indicate the batch size, the path to the directory of the validation images, the target size for the images, and to set the class mode to sparse. It is not necessary to shuffle the images in the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPF4X4VW8YJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size, \n",
        "                                                 directory=val_dir, \n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtqJO4Cv80y8",
        "colab_type": "text"
      },
      "source": [
        "**Create the CNN**\n",
        "\n",
        "We will create a convolutional neural network that consists of 3 convolution blocks. Each convolutional block contains a Conv2D layer followed by a max pool layer. The first convolutional block will have 16 filters, the second one will have 32 filters, and the third one will have 64 filters. All convolutional filters will be 3 x 3. All max pool layers will have a pool_size of (2, 2) .\n",
        "\n",
        "After the 3 convolutional blocks we will have a flatten layer followed by a fully connected layer with 512 units. The CNN will output class probabilities based on 5 classes which is done by the softmax activation function. All other layers will use a relu activation function. We will also add Dropout layers with a probability of 20%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjycXBbs7wG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_SHAPE,IMG_SHAPE, 3))) \n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjJjIQDb9gSa",
        "colab_type": "text"
      },
      "source": [
        "**Compile the Model**\n",
        "\n",
        "We will compile our model using the ADAM optimizer, the sparse cross entropy function as a loss function. We will look at training and validation accuracy on each epoch as we train our network, so we will also pass the metrics argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNp51UVm9jNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzJdp2A_-k85",
        "colab_type": "text"
      },
      "source": [
        "**Train the Model**\n",
        "\n",
        "We will train our model using the fit_generator function instead of the usual fit function. We have to use the fit_generator function because we are using the ImageDataGenerator class to generate batches of training and validation data for our model. Train the model for 80 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3pulwmI-wIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 80\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(train_data_gen.n / float(batch_size))),\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(val_data_gen.n / float(batch_size)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faOCUS2U-3la",
        "colab_type": "text"
      },
      "source": [
        "Plot Training and Validation Graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj-UivbE-4xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}